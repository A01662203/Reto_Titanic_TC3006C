{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Fare', 'Embarked_Cherbourg', 'Embarked_Queenstown',\n",
      "       'Embarked_Southampton', 'Alone', 'Large', 'Medium', 'Small', 'Female',\n",
      "       'Male', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'FemaleAge2', 'MaleAge6'],\n",
      "      dtype='object')\n",
      "✓ Ambos datasets tienen la misma cantidad de columnas\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "os.chdir('c:\\\\Users\\\\aesca\\\\Documents\\\\TEC\\\\Académico\\\\7mo semestre\\\\Reto_Titanic_TC3006C\\\\')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load train data using os path\n",
    "# train = pd.read_csv('./data/train_clean.csv')\n",
    "train = pd.read_csv('./data/train/train_clean.csv')\n",
    "# Load test data\n",
    "test = pd.read_csv('./data/test/test_clean.csv')\n",
    "\n",
    "# Drop the columns that are not needed in the train dataset\n",
    "x_train = train.drop('Survived', axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Drop the columns that are not needed in the test dataset\n",
    "x_test = test.drop('Survived', axis=1)\n",
    "y_test = test['Survived']\n",
    "\n",
    "# # Drop 'Age' column\n",
    "# x_train = x_train.drop('Age', axis=1)\n",
    "# x_test = x_test.drop('Age', axis=1)\n",
    "\n",
    "# Drop 'PassengerId' column\n",
    "x_train = x_train.drop('PassengerId', axis=1)\n",
    "x_test = x_test.drop('PassengerId', axis=1)\n",
    "\n",
    "# Print cols of each train and test dataset\n",
    "print(x_train.columns)\n",
    "if len(x_train.columns) == len(x_test.columns):\n",
    "    print('✓ Ambos datasets tienen la misma cantidad de columnas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10.0, 'penalty': 'l1'}\n",
      "True Positive: 264\n",
      "False Positive: 87\n",
      "True Negative: 462\n",
      "False Negative: 76\n",
      "Precision: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to find best parameters\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1., 10.],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced', solver='liblinear'), param_grid, cv=5, scoring='precision')\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Print True Positive, False Positive, True Negative, False Negative as variables\n",
    "y_pred = grid_search.predict(x_train)\n",
    "confusion_matrix_1 = confusion_matrix(y_train, y_pred)\n",
    "print(f'True Positive: {confusion_matrix_1[1][1]}')\n",
    "print(f'False Positive: {confusion_matrix_1[0][1]}')\n",
    "print(f'True Negative: {confusion_matrix_1[0][0]}')\n",
    "print(f'False Negative: {confusion_matrix_1[1][0]}')\n",
    "print(f'Precision: {(precision_score(y_train, y_pred).round(2))*100}%')\n",
    "\n",
    "# Delete variable y_pred\n",
    "del y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 81.0%\n",
      "Accuracy: 90.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict with best parameters\n",
    "y_pred = grid_search.predict(x_test)\n",
    "precision = grid_search.score(x_test, y_test)\n",
    "print(f'Precision: {precision.round(2)*100}%')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy.round(2)*100}%')\n",
    "\n",
    "\n",
    "# Confusion Matrix plot\n",
    "confusion_matrix_2 = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(confusion_matrix_2, annot=True, fmt='d')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_Cherbourg</th>\n",
       "      <th>Embarked_Queenstown</th>\n",
       "      <th>Embarked_Southampton</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Large</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Small</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>FemaleAge2</th>\n",
       "      <th>MaleAge6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024969</td>\n",
       "      <td>0.00343</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.322612</td>\n",
       "      <td>0.573956</td>\n",
       "      <td>-1.582713</td>\n",
       "      <td>-2.44827</td>\n",
       "      <td>0.516345</td>\n",
       "      <td>2.337855</td>\n",
       "      <td>-0.589944</td>\n",
       "      <td>1.085266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.747108</td>\n",
       "      <td>-0.756949</td>\n",
       "      <td>3.027357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age     Fare  Embarked_Cherbourg  Embarked_Queenstown  \\\n",
       "0 -0.024969  0.00343            0.094405                  0.0   \n",
       "\n",
       "   Embarked_Southampton     Alone     Large   Medium     Small    Female  \\\n",
       "0             -0.322612  0.573956 -1.582713 -2.44827  0.516345  2.337855   \n",
       "\n",
       "       Male  Pclass_1  Pclass_2  Pclass_3  FemaleAge2  MaleAge6  \n",
       "0 -0.589944  1.085266       0.0 -0.747108   -0.756949  3.027357  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model coefficients and the variable names to see which variables are important\n",
    "coef = grid_search.best_estimator_.coef_\n",
    "cols = x_train.columns\n",
    "# Combine the two arrays into a dataframe\n",
    "coef_df = pd.DataFrame(coef, columns=cols)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 12 features, but LogisticRegression is expecting 16 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fill a model predict with a male with an age of 25 that paid 100 for the ticket, traveled in Pclass 1 and embarked from Cherbourg, it should have 9 features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:499\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    498\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12 features, but LogisticRegression is expecting 16 features as input."
     ]
    }
   ],
   "source": [
    "# Fill a model predict with a male with an age of 25 that paid 100 for the ticket, traveled in Pclass 1 and embarked from Cherbourg, it should have 9 features\n",
    "print(grid_search.predict([[3, 32, 56, 0, 0, 1, 1, 0, 0, 0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_Cherbourg</th>\n",
       "      <th>Embarked_Queenstown</th>\n",
       "      <th>Embarked_Southampton</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Large</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Small</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.1417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.7958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.7875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>284</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>339</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>392</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.7958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>401</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>415</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>430</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>445</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>8.1125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>456</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>554</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>644</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>693</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>763</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>805</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>822</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>829</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>839</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass   Age     Fare  Embarked_Cherbourg  \\\n",
       "36            37         1       3  47.0   7.2292                 1.0   \n",
       "73            75         1       3  32.0  56.4958                 0.0   \n",
       "80            82         1       3  29.0   9.5000                 0.0   \n",
       "106          108         1       3  47.0   7.7750                 0.0   \n",
       "126          128         1       3  24.0   7.1417                 0.0   \n",
       "145          147         1       3  27.0   7.7958                 0.0   \n",
       "203          205         1       3  18.0   8.0500                 0.0   \n",
       "206          208         1       3  26.0  18.7875                 1.0   \n",
       "219          221         1       3  16.0   8.0500                 0.0   \n",
       "270          272         1       3  25.0   0.0000                 0.0   \n",
       "282          284         1       3  19.0   8.0500                 0.0   \n",
       "285          287         1       3  30.0   9.5000                 0.0   \n",
       "337          339         1       3  45.0   8.0500                 0.0   \n",
       "390          392         1       3  21.0   7.7958                 0.0   \n",
       "399          401         1       3  39.0   7.9250                 0.0   \n",
       "413          415         1       3  44.0   7.9250                 0.0   \n",
       "428          430         1       3  32.0   8.0500                 0.0   \n",
       "443          445         1       3  47.0   8.1125                 0.0   \n",
       "454          456         1       3  29.0   7.8958                 1.0   \n",
       "508          510         1       3  26.0  56.4958                 0.0   \n",
       "509          511         1       3  29.0   7.7500                 0.0   \n",
       "552          554         1       3  22.0   7.2250                 1.0   \n",
       "568          570         1       3  32.0   7.8542                 0.0   \n",
       "578          580         1       3  32.0   7.9250                 0.0   \n",
       "642          644         1       3  47.0  56.4958                 0.0   \n",
       "691          693         1       3  47.0  56.4958                 0.0   \n",
       "743          745         1       3  31.0   7.9250                 0.0   \n",
       "761          763         1       3  20.0   7.2292                 1.0   \n",
       "803          805         1       3  27.0   6.9750                 0.0   \n",
       "820          822         1       3  27.0   8.6625                 0.0   \n",
       "827          829         1       3  47.0   7.7500                 0.0   \n",
       "836          839         1       3  32.0  56.4958                 0.0   \n",
       "\n",
       "     Embarked_Queenstown  Embarked_Southampton  Alone  Large  Medium  Small  \\\n",
       "36                   0.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "73                   0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "80                   0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "106                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "126                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "145                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "203                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "206                  0.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "219                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "270                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "282                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "285                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "337                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "390                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "399                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "413                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "428                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "443                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "454                  0.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "508                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "509                  1.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "552                  0.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "568                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "578                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "642                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "691                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "743                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "761                  0.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "803                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "820                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "827                  1.0                   0.0    1.0    0.0     0.0    0.0   \n",
       "836                  0.0                   1.0    1.0    0.0     0.0    0.0   \n",
       "\n",
       "     Female  Male  \n",
       "36      0.0   1.0  \n",
       "73      0.0   1.0  \n",
       "80      0.0   1.0  \n",
       "106     0.0   1.0  \n",
       "126     0.0   1.0  \n",
       "145     0.0   1.0  \n",
       "203     0.0   1.0  \n",
       "206     0.0   1.0  \n",
       "219     0.0   1.0  \n",
       "270     0.0   1.0  \n",
       "282     0.0   1.0  \n",
       "285     0.0   1.0  \n",
       "337     0.0   1.0  \n",
       "390     0.0   1.0  \n",
       "399     0.0   1.0  \n",
       "413     0.0   1.0  \n",
       "428     0.0   1.0  \n",
       "443     0.0   1.0  \n",
       "454     0.0   1.0  \n",
       "508     0.0   1.0  \n",
       "509     0.0   1.0  \n",
       "552     0.0   1.0  \n",
       "568     0.0   1.0  \n",
       "578     0.0   1.0  \n",
       "642     0.0   1.0  \n",
       "691     0.0   1.0  \n",
       "743     0.0   1.0  \n",
       "761     0.0   1.0  \n",
       "803     0.0   1.0  \n",
       "820     0.0   1.0  \n",
       "827     0.0   1.0  \n",
       "836     0.0   1.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pclass = 1, male check if survived in train\n",
    "variable = train[(train['Male'] == 1)&(train['Survived'] == 1)&(train['Pclass'] == 3) & (train['Alone'] == 1)]\n",
    "print(len(variable))\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run section only if 'titanic_tree.pdf' is not in the directory\n",
    "\n",
    "if not os.path.exists('titanic_tree.pdf'):\n",
    "    # Decision Tree Clasiifier with graphviz\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import graphviz\n",
    "\n",
    "    # Create a Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Plot the tree with graphviz\n",
    "    dot_data = export_graphviz(clf, out_file=None, feature_names=x_train.columns, class_names=['Died', 'Survived'], filled=True, rounded=True, special_characters=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.render('titanic_tree')\n",
    "    graph.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
