{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto Titanic\n",
    "### Materia: TC3006C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "# Open test.csv and count the number of rows\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(df_test.shape[0])\n",
    "# Open train.csv and count the number of rows\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "print(df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId     0.000000\n",
      "Survived        0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            19.865320\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.000000\n",
      "Cabin          77.104377\n",
      "Embarked        0.224467\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get missing percentage per column in df_train\n",
    "missing_values = df_train.isnull().mean() * 100\n",
    "print(missing_values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket information feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ticket'] = df_train['Ticket'].replace('LINE', 'LINE 0')\n",
    "df_test['Ticket'] = df_test['Ticket'].replace('LINE', 'LINE 0')\n",
    "\n",
    "# Parse Ticket feature\n",
    "df_train['Ticket'] = df_train['Ticket'].apply(lambda x: x.replace('.','').replace('/','').lower())\n",
    "def get_prefix(ticket):\n",
    "    lead = ticket.split(' ')[0][0]\n",
    "    if lead.isalpha():\n",
    "        return ticket.split(' ')[0]\n",
    "    else:\n",
    "        return 'NoPrefix'\n",
    "    \n",
    "df_train['TicketPrefix'] = df_train['Ticket'].apply(lambda x: get_prefix(x))\n",
    "\n",
    "# Separate all ticket components\n",
    "df_train['Ticket_Number'] = df_train['Ticket'].apply(lambda x: int(x.split(' ')[-1])//1)\n",
    "df_train['Ticket_Length'] = df_train['Ticket_Number'].apply(lambda x : len(str(x)))\n",
    "df_train['Ticket_FirstDigit'] = df_train['Ticket_Number'].apply(lambda x : int(str(x)[0]))\n",
    "df_train['Ticket_Group'] = df_train['Ticket'].apply(lambda x: str(int(x.split(' ')[-1])//10))\n",
    "\n",
    "# Apply al separations for test data\n",
    "df_test['Ticket'] = df_test['Ticket'].apply(lambda x: x.replace('.','').replace('/','').lower())\n",
    "df_test['TicketPrefix'] = df_test['Ticket'].apply(lambda x: get_prefix(x))\n",
    "df_test['Ticket_Number'] = df_test['Ticket'].apply(lambda x: int(x.split(' ')[-1])//1)\n",
    "df_test['Ticket_Length'] = df_test['Ticket_Number'].apply(lambda x : len(str(x)))\n",
    "df_test['Ticket_FirstDigit'] = df_test['Ticket_Number'].apply(lambda x : int(str(x)[0]))\n",
    "df_test['Ticket_Group'] = df_test['Ticket'].apply(lambda x: str(int(x.split(' ')[-1])//10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ticket, Ticket_Number, Ticket_Length\n",
    "df_train = df_train.drop(columns=['Ticket', 'Ticket_Number', 'Ticket_Length'])\n",
    "df_test = df_test.drop(columns=['Ticket', 'Ticket_Number', 'Ticket_Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticket_FirstDigit</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>136</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>90</td>\n",
       "      <td>330</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>367</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticket_FirstDigit  0    1    2    3   4  5   6   7  8  9  All\n",
       "Pclass                                                       \n",
       "1                  0  192    4   14   0  4   2   0  0  0  216\n",
       "2                  0   22  136   23   0  1   0   2  0  0  184\n",
       "3                  4   17   90  330  15  4  12  13  3  3  491\n",
       "All                4  231  230  367  15  9  14  15  3  3  891"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_train['Pclass'],df_train['Ticket_FirstDigit'],margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a5' 'pc' 'stono2' 'NoPrefix' 'pp' 'ca' 'scparis' 'sca4' 'a4' 'sp' 'soc'\n",
      " 'wc' 'sotonoq' 'wep' 'stono' 'c' 'sop' 'fa' 'line' 'fcc' 'swpp' 'scow'\n",
      " 'ppp' 'sc' 'scah' 'as' 'sopp' 'fc' 'sotono2' 'casoton']\n"
     ]
    }
   ],
   "source": [
    "# Print the unique 'Prefix' in df_train\n",
    "print(df_train['TicketPrefix'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoPrefix    661\n",
      "pc           60\n",
      "ca           41\n",
      "a5           21\n",
      "sotonoq      15\n",
      "stono        12\n",
      "scparis      11\n",
      "wc           10\n",
      "a4            7\n",
      "stono2        6\n",
      "soc           6\n",
      "fcc           5\n",
      "c             5\n",
      "line          4\n",
      "wep           3\n",
      "scah          3\n",
      "sopp          3\n",
      "pp            3\n",
      "sotono2       2\n",
      "swpp          2\n",
      "ppp           2\n",
      "fc            1\n",
      "as            1\n",
      "scow          1\n",
      "sc            1\n",
      "sp            1\n",
      "fa            1\n",
      "sop           1\n",
      "sca4          1\n",
      "casoton       1\n",
      "Name: TicketPrefix, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each 'Prefix' in df_train\n",
    "print(df_train['TicketPrefix'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Drop cabin column due to the high number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset\n",
      "PassengerId           0.000000\n",
      "Survived              0.000000\n",
      "Pclass                0.000000\n",
      "Name                  0.000000\n",
      "Sex                   0.000000\n",
      "Age                  19.865320\n",
      "SibSp                 0.000000\n",
      "Parch                 0.000000\n",
      "Fare                  0.000000\n",
      "Embarked              0.224467\n",
      "TicketPrefix          0.000000\n",
      "Ticket_FirstDigit     0.000000\n",
      "Ticket_Group          0.000000\n",
      "dtype: float64\n",
      "**************************************************\n",
      "Test dataset\n",
      "PassengerId           0.000000\n",
      "Pclass                0.000000\n",
      "Name                  0.000000\n",
      "Sex                   0.000000\n",
      "Age                  20.574163\n",
      "SibSp                 0.000000\n",
      "Parch                 0.000000\n",
      "Fare                  0.239234\n",
      "Embarked              0.000000\n",
      "TicketPrefix          0.000000\n",
      "Ticket_FirstDigit     0.000000\n",
      "Ticket_Group          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop cabin column\n",
    "df_train = df_train.drop(columns=['Cabin'])\n",
    "df_test = df_test.drop(columns=['Cabin'])\n",
    "# Count the number of missing values in the df_train dataset\n",
    "print(\"Train dataset\")\n",
    "missing_values = df_train.isnull().mean() * 100\n",
    "print(missing_values)\n",
    "print(\"*\"*50)\n",
    "print(\"Test dataset\")\n",
    "missing_values = df_test.isnull().mean() * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a df of the df['Sex'] == 'male' and df['Survived'] == 1 \n",
    "male_survived = df_train[df_train['Sex'] == 'male']\n",
    "female_survived = df_train[df_train['Sex'] == 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId          0.000000\n",
      "Survived             0.000000\n",
      "Pclass               0.000000\n",
      "Name                 0.000000\n",
      "Sex                  0.000000\n",
      "Age                  0.000000\n",
      "SibSp                0.000000\n",
      "Parch                0.000000\n",
      "Fare                 0.000000\n",
      "Embarked             0.224467\n",
      "TicketPrefix         0.000000\n",
      "Ticket_FirstDigit    0.000000\n",
      "Ticket_Group         0.000000\n",
      "Title                0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate title from name column\n",
    "df_train['Title'] = df_train['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "df_test['Title'] = df_test['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "\n",
    "# For every title, obtain the mean of the age\n",
    "title_mean_age = df_train.groupby('Title')['Age'].mean()\n",
    "\n",
    "# Fill missing values in Age column with the mean of the title plus one standard deviation\n",
    "for title, mean_age in title_mean_age.items():\n",
    "    df_train.loc[(df_train['Age'].isnull()) & (df_train['Title'] == title), 'Age'] = mean_age + title_mean_age.std()\n",
    "    df_test.loc[(df_test['Age'].isnull()) & (df_test['Title'] == title), 'Age'] = mean_age + title_mean_age.std()\n",
    "\n",
    "# Round the age column to the nearest integer\n",
    "df_train['Age'] = df_train['Age'].round()\n",
    "df_test['Age'] = df_test['Age'].round()\n",
    "\n",
    "# Count the number of missing values in the df_train dataset\n",
    "missing_values = df_train.isnull().mean() * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupos SOLO, SMALL, MEDIUM, LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      SMALL\n",
      "1      SMALL\n",
      "2      ALONE\n",
      "3      SMALL\n",
      "4      ALONE\n",
      "       ...  \n",
      "886    ALONE\n",
      "887    ALONE\n",
      "888    SMALL\n",
      "889    ALONE\n",
      "890    ALONE\n",
      "Length: 889, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Family_Count_Tr  = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "Family_Count_Ts  = df_test['SibSp'] + df_train['Parch'] + 1\n",
    "\n",
    "def categorize_family_size(family_count):\n",
    "    if family_count == 1:\n",
    "        return 'ALONE'\n",
    "    elif family_count in [2, 3, 4]:\n",
    "        return 'SMALL'\n",
    "    elif family_count in [5, 6]:\n",
    "        return 'MEDIUM'\n",
    "    elif family_count in [7, 8, 11]:\n",
    "        return 'LARGE'\n",
    "    else:\n",
    "        return 'UNKNOWN'  # In case there are other sizes not covered\n",
    "\n",
    "# Apply the function to create a new column\n",
    "Family_Size_Tr= Family_Count_Tr .apply(categorize_family_size)\n",
    "Family_Size_Ts= Family_Count.apply(categorize_family_size)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(Family_Size)\n",
    "\n",
    "# Make a piovvot table grouped by the family size to analiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop embarked rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset\n",
      "PassengerId          0.0\n",
      "Survived             0.0\n",
      "Pclass               0.0\n",
      "Name                 0.0\n",
      "Sex                  0.0\n",
      "Age                  0.0\n",
      "SibSp                0.0\n",
      "Parch                0.0\n",
      "Fare                 0.0\n",
      "Embarked             0.0\n",
      "TicketPrefix         0.0\n",
      "Ticket_FirstDigit    0.0\n",
      "Ticket_Group         0.0\n",
      "Title                0.0\n",
      "dtype: float64\n",
      "**************************************************\n",
      "Test dataset\n",
      "PassengerId          0.000000\n",
      "Pclass               0.000000\n",
      "Name                 0.000000\n",
      "Sex                  0.000000\n",
      "Age                  0.000000\n",
      "SibSp                0.000000\n",
      "Parch                0.000000\n",
      "Fare                 0.239234\n",
      "Embarked             0.000000\n",
      "TicketPrefix         0.000000\n",
      "Ticket_FirstDigit    0.000000\n",
      "Ticket_Group         0.000000\n",
      "Title                0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop embarked rows with missing values\n",
    "df_train = df_train.dropna(subset=['Embarked'])\n",
    "\n",
    "# Count the number of missing values in the df_train dataset\n",
    "print(\"Train dataset\")\n",
    "missing_values = df_train.isnull().mean() * 100\n",
    "print(missing_values)\n",
    "print(\"*\"*50)\n",
    "print(\"Test dataset\")\n",
    "missing_values = df_test.isnull().mean() * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Drop rows with missing values in Fare column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset\n",
      "PassengerId          0.0\n",
      "Survived             0.0\n",
      "Pclass               0.0\n",
      "Name                 0.0\n",
      "Sex                  0.0\n",
      "Age                  0.0\n",
      "SibSp                0.0\n",
      "Parch                0.0\n",
      "Fare                 0.0\n",
      "Embarked             0.0\n",
      "TicketPrefix         0.0\n",
      "Ticket_FirstDigit    0.0\n",
      "Ticket_Group         0.0\n",
      "Title                0.0\n",
      "dtype: float64\n",
      "**************************************************\n",
      "Test dataset\n",
      "PassengerId          0.0\n",
      "Pclass               0.0\n",
      "Name                 0.0\n",
      "Sex                  0.0\n",
      "Age                  0.0\n",
      "SibSp                0.0\n",
      "Parch                0.0\n",
      "Fare                 0.0\n",
      "Embarked             0.0\n",
      "TicketPrefix         0.0\n",
      "Ticket_FirstDigit    0.0\n",
      "Ticket_Group         0.0\n",
      "Title                0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values in Fare column\n",
    "df_test = df_test.dropna(subset=['Fare'])\n",
    "\n",
    "# Count the number of missing values in the df_train dataset\n",
    "print(\"Train dataset\")\n",
    "missing_values = df_train.isnull().mean() * 100\n",
    "print(missing_values)\n",
    "print(\"*\"*50)\n",
    "print(\"Test dataset\")\n",
    "missing_values = df_test.isnull().mean() * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ticket'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop Ticket column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicket\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aesca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Ticket'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop Ticket column\n",
    "df_train = df_train.drop(columns=['Ticket'])\n",
    "df_test = df_test.drop(columns=['Ticket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment by age of passengers and using pivot tables to analyze the survival rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by age in intervals of 5 of passengers and using pivot tables to analyze the survival rate\n",
    "age_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85]\n",
    "age_labels = ['0-4', '5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
    "\n",
    "df_train['AgeGroup'] = pd.cut(df_train['Age'], bins=age_bins, labels=age_labels)\n",
    "df_test['AgeGroup'] = pd.cut(df_test['Age'], bins=age_bins, labels=age_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Contar las ocurrencias conjuntas de Group Age y Sex\n",
    "joint_counts = df_train.groupby(['Sex', 'AgeGroup']).size().unstack(fill_value=0)\n",
    "\n",
    "# Paso 2: Calcular la probabilidad condicional P(Group Age | Sex)\n",
    "conditional_probabilities = joint_counts.div(joint_counts.sum(axis=1), axis=0)\n",
    "print(conditional_probabilities*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar una columna donde sea el ['AgeGroup'] y el ['Sex'] en una sola columna\n",
    "df_train['AgeGroup_Sex'] = df_train['AgeGroup'].astype(str) + '_' + df_train['Sex']\n",
    "df_test['AgeGroup_Sex'] = df_test['AgeGroup'].astype(str) + '_' + df_test['Sex']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Contar las ocurrencias conjuntas de Sex, Group Age y Survived\n",
    "joint_counts_survived = df_train.groupby(['Sex', 'AgeGroup', 'Survived']).size().unstack(fill_value=0)\n",
    "\n",
    "# Paso 2: Calcular la probabilidad condicional P(Survived = 1 | Sex, Group Age)\n",
    "survived_counts = joint_counts_survived[1]\n",
    "conditional_probabilities_survived = survived_counts.div(joint_counts_survived.sum(axis=1))\n",
    "print(conditional_probabilities_survived*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate df['Surname'] column from df['Name'] until comma example: Braund, Mr. Owen Harris -> Braund\n",
    "df_train['Surname'] = df_train['Name'].str.extract('([A-Za-z]+),', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Surname'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When df['Surname'] appears more than once, it is considered a family and from the family depending on the df_train['age_group'] generate an extra column called ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subplot of every column in df_train as index for the pivot table\n",
    "df_train_analisis = df_train.drop(columns=['PassengerId', 'Name', 'Age','Survived', 'Fare'])\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Survival Rate characteristics of Male passengers')\n",
    "\n",
    "for i, col in enumerate(df_train_analisis.columns):\n",
    "    pivot = df_train.pivot_table(index=col, columns='Survived', aggfunc='size', fill_value=0)\n",
    "    # Survival rate by column\n",
    "    pivot['Survival Rate'] = pivot[1] / (pivot[0] + pivot[1])\n",
    "    pivot['Survival Rate'].plot(kind='bar', ax=axs[i//3, i%3], color='skyblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subplot of every column in df_train as index for the pivot table\n",
    "df_train_analisis = male_survived.drop(columns=['PassengerId', 'Name', 'Ticket', 'Age','Survived', 'Fare', 'Sex'])\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Survival Rate characteristics of Male passengers')\n",
    "\n",
    "for i, col in enumerate(df_train_analisis.columns):\n",
    "    pivot = male_survived.pivot_table(index=col, columns='Survived', aggfunc='size', fill_value=0)\n",
    "    # Survival rate by column\n",
    "    pivot['Survival Rate'] = pivot[1] / (pivot[0] + pivot[1])\n",
    "    pivot['Survival Rate'].plot(kind='bar', ax=axs[i//3, i%3], color='skyblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #Visuization: \n",
    "# pivot_class_survived = df_train.pivot_table(index='Embarked', columns='Survived', aggfunc='size', fill_value=0)\n",
    "\n",
    "# #Survival rate by class\n",
    "# pivot_class_survived['Survival Rate'] = pivot_class_survived[1] / (pivot_class_survived[0] + pivot_class_survived[1])\n",
    "# print(pivot_class_survived)\n",
    "\n",
    "# # Plotting the survival rate by class\n",
    "# pivot_class_survived['Survival Rate'].plot(kind='bar', color='skyblue')\n",
    "# plt.ylabel('Survival Rate')\n",
    "# plt.title('Survival Rate by Passenger Class')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subplot of every column in df_train as index for the pivot table\n",
    "df_train_analisis = female_survived.drop(columns=['PassengerId', 'Name', 'Ticket', 'Age', 'Fare', 'Survived', 'Sex'])\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Survival Rate characteristics of Female passengers')\n",
    "\n",
    "for i, col in enumerate(df_train_analisis.columns):\n",
    "    pivot = female_survived.pivot_table(index=col, columns='Survived', aggfunc='size', fill_value=0)\n",
    "    # Survival rate by column\n",
    "    pivot['Survival Rate'] = pivot[1] / (pivot[0] + pivot[1])\n",
    "    pivot['Survival Rate'].plot(kind='bar', ax=axs[i//3, i%3], color='skyblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los pasajeros que sobrevivieron\n",
    "survived = df_train[df_train['Survived'] == 1]\n",
    "\n",
    "# Crear el histograma directamente para 'Fare' de los que sobrevivieron\n",
    "plt.hist(df_train['Survived'], bins=10, edgecolor='black')\n",
    "\n",
    "# A√±adir t√≠tulos y etiquetas\n",
    "plt.title('Histograma de Fare para Pasajeros Sobrevivientes')\n",
    "plt.xlabel('Tarifa (Fare)')\n",
    "plt.ylabel('Cantidad de Sobrevivientes')\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph fare distribution\n",
    "df_train['Fare'].plot(kind='hist', bins=100, edgecolor='black')\n",
    "plt.title('Fare Distribution')\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Show the id of the passenger who paid the most\n",
    "print(df_train.loc[df_train['Fare'].idxmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
